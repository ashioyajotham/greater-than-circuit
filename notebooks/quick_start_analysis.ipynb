{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41fc33fd",
   "metadata": {},
   "source": [
    "# Greater Than Circuit Analysis - Quick Start\n",
    "\n",
    "This notebook provides a quick start guide for analyzing the greater than circuit in GPT-2 Small.\n",
    "\n",
    "**Acknowledgment**: This analysis builds upon the foundational work of Neel Nanda and the mechanistic interpretability community."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7ae98c",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d94877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "from model_setup import ModelSetup\n",
    "from prompt_design import PromptGenerator\n",
    "from activation_patching import ActivationPatcher\n",
    "from circuit_analysis import CircuitAnalyzer\n",
    "from visualization import CircuitVisualizer\n",
    "from circuit_validation import CircuitValidator\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538c68a0",
   "metadata": {},
   "source": [
    "## 1. Model Setup\n",
    "\n",
    "Load GPT-2 Small using TransformerLens for mechanistic interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e17a260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model setup\n",
    "setup = ModelSetup()\n",
    "model = setup.load_model()\n",
    "\n",
    "# Print model information\n",
    "setup.print_model_info()\n",
    "\n",
    "# Test basic functionality\n",
    "test_result = setup.test_model_basic(\"5 > 3:\")\n",
    "print(f\"\\nTest completion: {test_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f98d6db",
   "metadata": {},
   "source": [
    "## 2. Generate Test Data\n",
    "\n",
    "Create balanced datasets of numerical comparison examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c2f939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize prompt generator\n",
    "generator = PromptGenerator(seed=42)\n",
    "\n",
    "# Generate balanced test examples\n",
    "test_examples = generator.generate_balanced_dataset(n_examples=100)\n",
    "\n",
    "# Show statistics\n",
    "generator.print_statistics(test_examples)\n",
    "\n",
    "# Display sample examples\n",
    "print(\"\\nSample Examples:\")\n",
    "for i, example in enumerate(test_examples[:5]):\n",
    "    print(f\"{i+1}. {example.prompt_text} -> {example.answer_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72135a5",
   "metadata": {},
   "source": [
    "## 3. Create Prompt Pairs for Patching\n",
    "\n",
    "Generate clean and corrupted prompt pairs for activation patching experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b61e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompt pairs for activation patching\n",
    "prompt_pairs = generator.create_prompt_pairs(n_pairs=10)\n",
    "\n",
    "# Show example pairs\n",
    "print(\"Example Clean vs Corrupted Pairs:\")\n",
    "for i, (clean, corrupted) in enumerate(prompt_pairs[:3]):\n",
    "    print(f\"\\nPair {i+1}:\")\n",
    "    print(f\"  Clean:     {clean.prompt_text} -> {clean.answer_text}\")\n",
    "    print(f\"  Corrupted: {corrupted.prompt_text} -> {corrupted.answer_text}\")\n",
    "\n",
    "# Select first pair for detailed analysis\n",
    "clean_example, corrupted_example = prompt_pairs[0]\n",
    "print(f\"\\nUsing for analysis:\")\n",
    "print(f\"Clean: {clean_example.prompt_text}\")\n",
    "print(f\"Corrupted: {corrupted_example.prompt_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85fb71c",
   "metadata": {},
   "source": [
    "## 4. Baseline Model Performance\n",
    "\n",
    "Test the model's baseline accuracy on the greater than task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04617d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize validator for baseline testing\n",
    "patcher = ActivationPatcher(model)\n",
    "analyzer = CircuitAnalyzer(model)\n",
    "validator = CircuitValidator(model, generator, patcher, analyzer)\n",
    "\n",
    "# Test baseline accuracy\n",
    "baseline_result = validator.validate_baseline_accuracy(test_examples[:50])\n",
    "\n",
    "print(f\"Baseline Accuracy: {baseline_result.accuracy:.3f}\")\n",
    "print(f\"Correct Predictions: {baseline_result.correct_predictions}/{baseline_result.total_examples}\")\n",
    "print(f\"Precision: {baseline_result.precision:.3f}\")\n",
    "print(f\"Recall: {baseline_result.recall:.3f}\")\n",
    "print(f\"F1 Score: {baseline_result.f1_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfd5382",
   "metadata": {},
   "source": [
    "## 5. Activation Patching Experiment\n",
    "\n",
    "Run activation patching to identify critical components in the greater than circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd378a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize prompt pair\n",
    "clean_tokens = model.to_tokens(clean_example.prompt_text + \" \")\n",
    "corrupted_tokens = model.to_tokens(corrupted_example.prompt_text + \" \")\n",
    "\n",
    "print(f\"Clean tokens shape: {clean_tokens.shape}\")\n",
    "print(f\"Corrupted tokens shape: {corrupted_tokens.shape}\")\n",
    "print(f\"Clean tokens: {model.to_str_tokens(clean_tokens[0])}\")\n",
    "print(f\"Corrupted tokens: {model.to_str_tokens(corrupted_tokens[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5be8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive activation patching (this may take a few minutes)\n",
    "print(\"Running activation patching experiments...\")\n",
    "\n",
    "# Patch attention heads specifically\n",
    "attention_results = patcher.patch_attention_heads(\n",
    "    corrupted_tokens=corrupted_tokens,\n",
    "    clean_tokens=clean_tokens,\n",
    "    positions=[-1]  # Focus on last token position\n",
    ")\n",
    "\n",
    "print(f\"Completed {len(attention_results)} attention head patching experiments\")\n",
    "\n",
    "# Find top components\n",
    "top_components = patcher.find_critical_components(\n",
    "    attention_results, \n",
    "    threshold=0.05, \n",
    "    top_k=10\n",
    ")\n",
    "\n",
    "print(f\"\\nTop {len(top_components)} critical attention heads:\")\n",
    "for i, result in enumerate(top_components):\n",
    "    print(f\"{i+1:2d}. Layer {result.layer:2d}, Head {result.head:2d}: Effect = {result.effect_size:+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d337c1",
   "metadata": {},
   "source": [
    "## 6. Circuit Analysis\n",
    "\n",
    "Analyze the patching results to identify and understand the circuit structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27e3392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify circuit components\n",
    "circuit_components = analyzer.identify_circuit_components(\n",
    "    attention_results, \n",
    "    importance_threshold=0.05,\n",
    "    top_k=15\n",
    ")\n",
    "\n",
    "print(f\"Identified {len(circuit_components)} circuit components:\")\n",
    "for name, comp in circuit_components.items():\n",
    "    print(f\"  {name}: Layer {comp.layer}, Importance {comp.importance_score:.3f} ({comp.component_type})\")\n",
    "\n",
    "# Analyze layer contributions\n",
    "layer_contributions = analyzer.analyze_layer_contributions(attention_results)\n",
    "print(f\"\\nLayer contributions:\")\n",
    "for layer, contrib in sorted(layer_contributions.items()):\n",
    "    print(f\"  Layer {layer}: {contrib:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fcb157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive circuit summary\n",
    "circuit_summary = analyzer.create_circuit_summary(attention_results)\n",
    "\n",
    "print(\"Circuit Summary:\")\n",
    "print(f\"  Total Components: {circuit_summary['circuit_overview']['total_components']}\")\n",
    "print(f\"  Circuit Depth: {circuit_summary['circuit_depth']} layers\")\n",
    "print(f\"  Most Important Layer: {circuit_summary['circuit_overview']['most_important_layer']}\")\n",
    "print(f\"  Attention Heads: {len(circuit_summary['circuit_overview']['attention_heads'])}\")\n",
    "print(f\"  Important Heads: {circuit_summary['circuit_overview']['attention_heads'][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3271608",
   "metadata": {},
   "source": [
    "## 7. Visualization\n",
    "\n",
    "Create visualizations to understand the circuit structure and behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a10363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize visualizer\n",
    "visualizer = CircuitVisualizer(output_dir=\"../results\")\n",
    "\n",
    "# Plot patching results\n",
    "fig1 = visualizer.plot_patching_results(\n",
    "    attention_results,\n",
    "    title=\"Attention Head Patching Results\",\n",
    "    top_k=15\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Plot layer importance\n",
    "fig2 = visualizer.plot_layer_importance(\n",
    "    layer_contributions,\n",
    "    title=\"Layer Contributions to Greater Than Circuit\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffb6423",
   "metadata": {},
   "source": [
    "## 8. Attention Pattern Analysis\n",
    "\n",
    "Examine attention patterns in the most important heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1c95e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top attention heads for analysis\n",
    "top_heads = [(comp.layer, comp.head) for comp in circuit_components.values() \n",
    "             if comp.head is not None][:3]\n",
    "\n",
    "print(f\"Analyzing attention patterns for heads: {top_heads}\")\n",
    "\n",
    "# Analyze attention patterns\n",
    "attention_patterns = analyzer.analyze_attention_patterns(\n",
    "    tokens=clean_tokens,\n",
    "    target_heads=top_heads\n",
    ")\n",
    "\n",
    "# Get token labels for visualization\n",
    "token_labels = model.to_str_tokens(clean_tokens[0])\n",
    "\n",
    "# Visualize attention patterns\n",
    "if attention_patterns:\n",
    "    fig3 = visualizer.plot_attention_patterns(\n",
    "        attention_patterns,\n",
    "        token_labels,\n",
    "        title=\"Attention Patterns in Critical Heads\"\n",
    "    )\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No attention patterns captured - check hook names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde48212",
   "metadata": {},
   "source": [
    "## 9. Circuit Validation\n",
    "\n",
    "Validate the identified circuit through various tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21bafa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test circuit necessity\n",
    "necessity_result = validator.validate_circuit_necessity(\n",
    "    test_examples[:20],\n",
    "    circuit_components,\n",
    "    ablation_type=\"zero_ablation\"\n",
    ")\n",
    "\n",
    "print(f\"Circuit Necessity Test:\")\n",
    "print(f\"  Baseline Accuracy: {necessity_result.details['baseline_accuracy']:.3f}\")\n",
    "print(f\"  Ablated Accuracy: {necessity_result.details['ablated_accuracy']:.3f}\")\n",
    "print(f\"  Necessity Score: {necessity_result.details['necessity_score']:.3f}\")\n",
    "\n",
    "# Test robustness\n",
    "robustness_results = validator.validate_robustness(\n",
    "    test_examples[:30],\n",
    "    perturbation_types=[\"edge_cases\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nRobustness Test:\")\n",
    "for result in robustness_results:\n",
    "    print(f\"  {result.test_name}: {result.accuracy:.3f} (drop: {result.details['robustness_drop']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258548f0",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps\n",
    "\n",
    "Summarize findings and suggest further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cfbbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"GREATER THAN CIRCUIT ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä PERFORMANCE METRICS:\")\n",
    "print(f\"   Baseline Accuracy: {baseline_result.accuracy:.1%}\")\n",
    "print(f\"   Circuit Components: {len(circuit_components)}\")\n",
    "print(f\"   Circuit Depth: {circuit_summary['circuit_depth']} layers\")\n",
    "\n",
    "print(f\"\\nüß† KEY COMPONENTS:\")\n",
    "for i, (name, comp) in enumerate(list(circuit_components.items())[:5]):\n",
    "    print(f\"   {i+1}. {name}: Layer {comp.layer}, Importance {comp.importance_score:.3f}\")\n",
    "\n",
    "print(f\"\\nüéØ CRITICAL LAYERS:\")\n",
    "sorted_layers = sorted(layer_contributions.items(), key=lambda x: x[1], reverse=True)\n",
    "for layer, contrib in sorted_layers[:3]:\n",
    "    print(f\"   Layer {layer}: {contrib:.3f} average contribution\")\n",
    "\n",
    "print(f\"\\n‚úÖ VALIDATION RESULTS:\")\n",
    "print(f\"   Circuit Necessity: {necessity_result.details['necessity_score']:.3f}\")\n",
    "if robustness_results:\n",
    "    print(f\"   Robustness: {robustness_results[0].accuracy:.3f}\")\n",
    "\n",
    "print(f\"\\nüîç NEXT STEPS:\")\n",
    "print(f\"   1. Analyze individual component behavior in detail\")\n",
    "print(f\"   2. Test on larger and more diverse datasets\")\n",
    "print(f\"   3. Compare with circuits for related tasks (less than, equal to)\")\n",
    "print(f\"   4. Investigate cross-model generalization\")\n",
    "print(f\"   5. Develop mechanistic hypotheses for how the circuit works\")\n",
    "\n",
    "print(f\"\\nüôè Acknowledgment: This analysis builds upon the foundational work\")\n",
    "print(f\"   of Neel Nanda and the mechanistic interpretability community.\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
